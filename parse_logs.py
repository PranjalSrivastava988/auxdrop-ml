#!/usr/bin/env python3
"""
Parse log files generated by run_all.py to extract experiment results
and compile them into a summary CSV file.
"""
import re
import glob
import pandas as pd
import os
from pathlib import Path
import argparse


def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Parse experiment log files")
    parser.add_argument(
        "--log-dir", type=str, default="logs", help="Directory containing log files"
    )
    parser.add_argument(
        "--output", type=str, default="summary_results.csv", help="Output CSV filename"
    )
    return parser.parse_args()


def main():
    args = parse_args()

    # Find all log files in the specified directory
    log_dir = Path(args.log_dir)
    log_files = list(log_dir.glob("results_*.log"))

    if not log_files:
        print(f"No log files found in {log_dir}. Exiting.")
        return

    # Compile regex patterns once for better performance
    filename_pattern = re.compile(
        r"results_(?P<data>\w+)_(?P<stream>\w+)_(?P<model>.+)\.log"
    )
    metrics_pattern = re.compile(
        r"The mean error in the\s+(?P<data2>\w+)\s+dataset.*?is\s+(?P<mean>[\d\.]+)\s+and the standard deviation is\s+(?P<std>[\d\.]+)",
        re.IGNORECASE,
    )

    # Process log files
    results = []
    for path in log_files:
        filename = path.name
        match_filename = filename_pattern.match(filename)
        if not match_filename:
            continue

        info = match_filename.groupdict()

        # Read log file content efficiently
        try:
            with open(path, "r", encoding="utf-8") as f:
                content = f.read()

            match_metrics = metrics_pattern.search(content)
            if not match_metrics:
                continue

            metrics = match_metrics.groupdict()

            # Add result to collection
            results.append(
                {
                    "dataset": info["data"],
                    "stream_type": info["stream"],
                    "model": info["model"],
                    "mean_error": float(metrics["mean"]),
                    "std_deviation": float(metrics["std"]),
                }
            )
        except Exception as e:
            print(f"Error processing {path}: {e}")

    # Create DataFrame and sort results
    if not results:
        print("No valid results found in log files.")
        return

    df = pd.DataFrame(results)
    df = df.sort_values(["dataset", "stream_type", "model"]).reset_index(drop=True)

    # Print preview with improved formatting
    pd.set_option("display.max_rows", 10)
    pd.set_option("display.width", 120)
    print(f"\nFound {len(df)} results:")
    print(df)

    # Save to CSV
    output_path = Path(args.output)
    df.to_csv(output_path, index=False)
    print(f"\nSaved summary to {output_path}")


if __name__ == "__main__":
    main()
